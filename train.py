"""ä¼˜åŒ–åçš„è®­ç»ƒè„šæœ¬ - å®Œæ•´ç‰ˆï¼ˆæ”¯æŒæ–­ç‚¹ç»­è®­å’Œçµæ´»çš„å­¦ä¹ ç‡ç­–ç•¥ï¼‰

åœºæ™¯ 1ï¼šåˆå§‹è®­ç»ƒ
python train.py \
    --scheduler_type onecycle \
    --num_epochs 300 \
    --lr 2e-4

    
åœºæ™¯ 2ï¼šå»¶é•¿è®­ç»ƒ
python train.py \
    --resume checkpoints/best_model.pth \
    --num_epochs 500 \
    --reset_scheduler  # é‡æ–°å¼€å§‹å­¦ä¹ ç‡è®¡åˆ’

    
åœºæ™¯ 3ï¼šä½¿ç”¨å‘¨æœŸæ€§é‡å¯ï¼ˆæ¨èï¼‰
# åˆå§‹è®­ç»ƒ
python train.py --scheduler_type cosine_restart --cosine_t0 50

# ç»§ç»­è®­ç»ƒï¼ˆè‡ªåŠ¨é‡å¯å­¦ä¹ ç‡ï¼‰
python train.py \
    --resume checkpoints/best_model.pth \
    --scheduler_type cosine_restart \
    --num_epochs 500



åœºæ™¯ 4ï¼šå¾®è°ƒ
python train.py \
    --resume checkpoints/best_model.pth \
    --finetune \
    --finetune_lr_ratio 0.1 \
    --num_epochs 350


"""

import argparse
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau, ExponentialLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.amp import autocast, GradScaler
from pathlib import Path
from tqdm import tqdm
import numpy as np
import sys
import json

from data import SketchColorPairDataset, check_and_download_dataset
from models import AdvancedFlowMatchingModel
from utils import get_transforms, visualize_pairs


class PerceptualLoss(nn.Module):
    """æ„ŸçŸ¥æŸå¤±ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„ VGG ç‰¹å¾"""
    def __init__(self):
        super().__init__()
        # ä½¿ç”¨ VGG16 çš„å‰å‡ å±‚
        from torchvision.models import vgg16, VGG16_Weights
        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features
        
        self.layers = nn.ModuleList([
            vgg[:4],   # relu1_2
            vgg[4:9],  # relu2_2
            vgg[9:16], # relu3_3
        ])
        
        # å†»ç»“å‚æ•°
        for param in self.parameters():
            param.requires_grad = False
        
        self.weights = [1.0, 1.0, 1.0]
    
    def forward(self, pred, target):
        loss = 0.0
        
        for i, layer in enumerate(self.layers):
            pred = layer(pred)
            target = layer(target)
            loss += self.weights[i] * nn.functional.l1_loss(pred, target)
        
        return loss


class EMA:
    """æŒ‡æ•°ç§»åŠ¨å¹³å‡ (Exponential Moving Average)
    
    ç”¨äºå¹³æ»‘æ¨¡å‹æƒé‡ï¼Œæå‡ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§
    """
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        # åˆå§‹åŒ– shadow å‚æ•°
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        """æ›´æ–° shadow å‚æ•°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        """åº”ç”¨ shadow å‚æ•°åˆ°æ¨¡å‹ï¼ˆç”¨äºéªŒè¯/æ¨ç†ï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]
    
    def restore(self):
        """æ¢å¤åŸå§‹å‚æ•°ï¼ˆç»§ç»­è®­ç»ƒï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


def save_checkpoint(model, optimizer, scheduler, scaler, ema, epoch, 
                   train_loss, val_loss, args, save_path, is_best=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        scaler: æ¢¯åº¦ç¼©æ”¾å™¨
        ema: EMA å¯¹è±¡
        epoch: å½“å‰ epoch
        train_loss: è®­ç»ƒæŸå¤±
        val_loss: éªŒè¯æŸå¤±
        args: è®­ç»ƒå‚æ•°
        save_path: ä¿å­˜è·¯å¾„
        is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
    """
    # éœ€è¦ä¿å­˜çš„æ¨¡å‹æ¶æ„å‚æ•°ï¼ˆç”¨äºé‡å»ºæ¨¡å‹ï¼‰
    model_config = {
        'base_channels': args.base_channels,
        'time_emb_dim': args.time_emb_dim,
        'num_heads': args.num_heads,
        'dropout': args.dropout,
        'stochastic_depth': args.stochastic_depth,
    }
    
    # éœ€è¦ä¿å­˜çš„è®­ç»ƒå‚æ•°ï¼ˆç”¨äºæ¢å¤è®­ç»ƒçŠ¶æ€ï¼‰
    training_config = {
        'batch_size': args.batch_size,
        'image_size': args.image_size,
        'lr': args.lr,
        'accumulation_steps': args.accumulation_steps,
        'use_ema': args.use_ema,
        'sketch_method': args.sketch_method,
        'num_workers': args.num_workers,
        'scheduler_type': args.scheduler_type,
    }
    
    save_dict = {
        # è®­ç»ƒçŠ¶æ€
        'epoch': epoch,
        'train_loss': train_loss,
        'val_loss': val_loss,
        'is_best': is_best,
        
        # æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
        'scaler_state_dict': scaler.state_dict(),
        
        # é…ç½®
        'model_config': model_config,
        'training_config': training_config,
        'all_args': vars(args),  # ä¿å­˜æ‰€æœ‰å‚æ•°ä»¥ä¾›å‚è€ƒ
    }
    
    # ä¿å­˜ EMA
    if ema is not None:
        save_dict['ema_shadow'] = ema.shadow
        save_dict['ema_decay'] = ema.decay
    
    torch.save(save_dict, save_path)
    
    # åŒæ—¶ä¿å­˜ä¸€ä»½å¯è¯»çš„é…ç½®æ–‡ä»¶
    config_path = save_path.parent / f"{save_path.stem}_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump({
            'epoch': epoch,
            'train_loss': float(train_loss),
            'val_loss': float(val_loss),
            'model_config': model_config,
            'training_config': training_config,
        }, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")


def load_checkpoint(checkpoint_path, device='cuda'):
    """åŠ è½½æ£€æŸ¥ç‚¹å¹¶è¿”å›é…ç½®
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡
        
    Returns:
        checkpoint: æ£€æŸ¥ç‚¹å­—å…¸
        model_config: æ¨¡å‹é…ç½®
        training_config: è®­ç»ƒé…ç½®
    """
    print(f"\n{'='*70}")
    print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½: {checkpoint_path}")
    print(f"{'='*70}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # æå–é…ç½®
    model_config = checkpoint.get('model_config', {})
    training_config = checkpoint.get('training_config', {})
    
    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿¡æ¯
    print(f"æ£€æŸ¥ç‚¹ä¿¡æ¯:")
    print(f"  Epoch: {checkpoint.get('epoch', 'Unknown')}")
    print(f"  è®­ç»ƒæŸå¤±: {checkpoint.get('train_loss', 'Unknown'):.4f}")
    print(f"  éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 'Unknown'):.4f}")
    print(f"  æ˜¯å¦æœ€ä½³: {checkpoint.get('is_best', False)}")
    
    print(f"\næ¨¡å‹é…ç½®:")
    for key, value in model_config.items():
        print(f"  {key}: {value}")
    
    print(f"\nè®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        print(f"  {key}: {value}")
    
    print(f"{'='*70}\n")
    
    return checkpoint, model_config, training_config


def verify_config_match(current_args, saved_config, config_type='model'):
    """éªŒè¯é…ç½®æ˜¯å¦åŒ¹é…
    
    Args:
        current_args: å½“å‰å‚æ•°
        saved_config: ä¿å­˜çš„é…ç½®
        config_type: é…ç½®ç±»å‹ ('model' æˆ– 'training')
    
    Returns:
        bool: æ˜¯å¦åŒ¹é…
        list: ä¸åŒ¹é…çš„å‚æ•°åˆ—è¡¨
    """
    mismatches = []
    
    for key, saved_value in saved_config.items():
        current_value = getattr(current_args, key, None)
        if current_value is not None and current_value != saved_value:
            mismatches.append({
                'param': key,
                'current': current_value,
                'saved': saved_value
            })
    
    return len(mismatches) == 0, mismatches


def create_scheduler(optimizer, args, train_loader, start_epoch=1):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        args: å‚æ•°
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        start_epoch: èµ·å§‹ epochï¼ˆç”¨äºè®¡ç®—å‰©ä½™ epochï¼‰
    
    Returns:
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_needs_metric: æ˜¯å¦éœ€è¦ä¼ å…¥ metricï¼ˆReduceLROnPlateauï¼‰
    """
    remaining_epochs = args.num_epochs - start_epoch + 1
    steps_per_epoch = len(train_loader) // args.accumulation_steps
    
    scheduler_needs_metric = False
    
    if args.scheduler_type == 'onecycle':
        scheduler = OneCycleLR(
            optimizer,
            max_lr=args.lr * 1.5,
            epochs=remaining_epochs,
            steps_per_epoch=steps_per_epoch,
            pct_start=0.15,
            anneal_strategy='cos',
            div_factor=1,
            final_div_factor=1e4
        )
        print(f"âœ“ OneCycleLR: {remaining_epochs} epochs, {steps_per_epoch} steps/epoch")
        print(f"  max_lr: {args.lr * 1.5:.2e}, final_lr: {args.lr * 1.5 / 1e4:.2e}")
    
    elif args.scheduler_type == 'cosine_restart':
        scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=args.cosine_t0,
            T_mult=args.cosine_tmult,
            eta_min=args.min_lr
        )
        print(f"âœ“ CosineAnnealingWarmRestarts: T_0={args.cosine_t0}, T_mult={args.cosine_tmult}")
        print(f"  eta_min: {args.min_lr:.2e}")
    
    elif args.scheduler_type == 'reduce_on_plateau':
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=args.plateau_factor,
            patience=args.plateau_patience,
            min_lr=args.min_lr,
            verbose=True
        )
        scheduler_needs_metric = True
        print(f"âœ“ ReduceLROnPlateau: factor={args.plateau_factor}, patience={args.plateau_patience}")
        print(f"  min_lr: {args.min_lr:.2e}")
    
    elif args.scheduler_type == 'exponential':
        scheduler = ExponentialLR(
            optimizer,
            gamma=args.exp_gamma
        )
        print(f"âœ“ ExponentialLR: gamma={args.exp_gamma}")
        print(f"  æ¯ epoch å­¦ä¹ ç‡è¡°å‡åˆ°åŸæ¥çš„ {args.exp_gamma:.2%}")
    
    elif args.scheduler_type == 'none':
        scheduler = None
        print(f"âœ“ ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå›ºå®šå­¦ä¹ ç‡: {args.lr:.2e}")
    
    else:
        raise ValueError(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {args.scheduler_type}")
    
    return scheduler, scheduler_needs_metric


def train_one_epoch(model, loader, optimizer, scheduler, scaler, device, epoch, 
                   perceptual_loss, ema, accumulation_steps=1, 
                   scheduler_needs_metric=False, writer=None):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    total_mse_loss = 0
    total_perceptual_loss = 0
    
    pbar = tqdm(loader, desc=f"Epoch {epoch}")
    optimizer.zero_grad()
    
    for i, (sketch, color) in enumerate(pbar):
        sketch = sketch.to(device)
        color = color.to(device)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
            # Flow Matching è®­ç»ƒ
            # ä½¿ç”¨é‡è¦æ€§é‡‡æ ·ï¼šåœ¨ä¸­é—´æ—¶é—´æ­¥é‡‡æ ·æ›´å¤š
            t = torch.rand(sketch.size(0), 1, device=device)
            t = torch.sigmoid(torch.randn_like(t))
            
            noise = torch.randn_like(color)
            noisy_color = t.view(-1, 1, 1, 1) * color + (1 - t.view(-1, 1, 1, 1)) * noise
            
            pred_velocity = model(sketch, noisy_color, t)
            target_velocity = color - noise
            
            # MSE æŸå¤±
            mse_loss = nn.functional.mse_loss(pred_velocity, target_velocity)
            
            # æ„ŸçŸ¥æŸå¤±ï¼ˆä»…åœ¨åæœŸè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
            if epoch > 10:
                # ä»é€Ÿåº¦åœºé‡å»ºå›¾åƒ
                pred_color = noisy_color + pred_velocity * (1 - t.view(-1, 1, 1, 1))
                pred_color = torch.clamp(pred_color, -1, 1)
                
                # å½’ä¸€åŒ–åˆ° [0, 1] ç”¨äº VGG
                pred_color_norm = (pred_color + 1) / 2
                color_norm = (color + 1) / 2
                
                perc_loss = perceptual_loss(pred_color_norm, color_norm)
                loss = mse_loss + 0.1 * perc_loss
                
                total_perceptual_loss += perc_loss.item()
            else:
                loss = mse_loss
                perc_loss = torch.tensor(0.0)
        
        # æ¢¯åº¦ç¼©æ”¾å’Œç´¯ç§¯
        scaler.scale(loss / accumulation_steps).backward()
        
        if (i + 1) % accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            scaler.step(optimizer)
            scaler.update()
            
            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆé™¤äº† ReduceLROnPlateauï¼‰
            if scheduler is not None and not scheduler_needs_metric:
                scheduler.step()
            
            optimizer.zero_grad()
            
            # æ›´æ–° EMA
            if ema is not None:
                ema.update()
        
        total_loss += loss.item()
        total_mse_loss += mse_loss.item()
        
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'mse': f'{mse_loss.item():.4f}',
            'perc': f'{perc_loss.item():.4f}' if epoch > 10 else 'N/A',
            'lr': f'{optimizer.param_groups[0]["lr"]:.6f}'
        })
        
        # è®°å½•åˆ° TensorBoard
        if writer is not None and i % 50 == 0:
            global_step = (epoch - 1) * len(loader) + i
            writer.add_scalar('train/batch_loss', loss.item(), global_step)
            writer.add_scalar('train/mse_loss', mse_loss.item(), global_step)
            if epoch > 10:
                writer.add_scalar('train/perceptual_loss', perc_loss.item(), global_step)
    
    avg_loss = total_loss / len(loader)
    avg_mse = total_mse_loss / len(loader)
    avg_perc = total_perceptual_loss / len(loader) if epoch > 10 else 0
    
    if writer is not None:
        writer.add_scalar('train/epoch_loss', avg_loss, epoch)
        writer.add_scalar('train/epoch_mse', avg_mse, epoch)
        if epoch > 10:
            writer.add_scalar('train/epoch_perceptual', avg_perc, epoch)
        writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)
    
    return avg_loss


@torch.no_grad()
def validate(model, loader, device, perceptual_loss, epoch=0, writer=None):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    total_mse_loss = 0
    total_perceptual_loss = 0
    
    for sketch, color in tqdm(loader, desc="éªŒè¯"):
        sketch = sketch.to(device)
        color = color.to(device)
        
        # ä½¿ç”¨å›ºå®šçš„æ—¶é—´æ­¥è¿›è¡ŒéªŒè¯
        t = torch.ones(sketch.size(0), 1, device=device) * 0.5
        noise = torch.randn_like(color)
        noisy_color = 0.5 * color + 0.5 * noise
        
        pred_velocity = model(sketch, noisy_color, t)
        target_velocity = color - noise
        
        mse_loss = nn.functional.mse_loss(pred_velocity, target_velocity)
        
        # æ„ŸçŸ¥æŸå¤±
        pred_color = noisy_color + pred_velocity * 0.5
        pred_color = torch.clamp(pred_color, -1, 1)
        
        pred_color_norm = (pred_color + 1) / 2
        color_norm = (color + 1) / 2
        perc_loss = perceptual_loss(pred_color_norm, color_norm)
        
        loss = mse_loss + 0.1 * perc_loss
        
        total_loss += loss.item()
        total_mse_loss += mse_loss.item()
        total_perceptual_loss += perc_loss.item()
    
    avg_loss = total_loss / len(loader)
    avg_mse = total_mse_loss / len(loader)
    avg_perc = total_perceptual_loss / len(loader)
    
    if writer is not None:
        writer.add_scalar('val/loss', avg_loss, epoch)
        writer.add_scalar('val/mse_loss', avg_mse, epoch)
        writer.add_scalar('val/perceptual_loss', avg_perc, epoch)
    
    return avg_loss


def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒç®€ç¬”ç”»ä¸Šè‰²æ¨¡å‹")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, default="datasets/anime_faces")
    parser.add_argument("--batch_size", type=int, default=96)
    parser.add_argument("--image_size", type=int, default=32)
    parser.add_argument("--sketch_method", type=str, default="canny")
    parser.add_argument("--num_workers", type=int, default=16)
    
    # æ¨¡å‹å‚æ•°ï¼ˆå½±å“æ¨¡å‹æ¶æ„ï¼‰
    parser.add_argument("--base_channels", type=int, default=96)
    parser.add_argument("--time_emb_dim", type=int, default=384)
    parser.add_argument("--num_heads", type=int, default=4)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--stochastic_depth", type=float, default=0.1)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--num_epochs", type=int, default=500)
    parser.add_argument("--lr", type=float, default=2e-4)
    parser.add_argument("--accumulation_steps", type=int, default=4)
    parser.add_argument("--use_ema", action="store_true", default=True)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    parser.add_argument("--scheduler_type", type=str, 
                       default="onecycle",
                       choices=["onecycle", "cosine_restart", "reduce_on_plateau", "exponential", "none"],
                       help="å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹")
    parser.add_argument("--reset_scheduler", action="store_true",
                       help="é‡ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆç”¨äºå»¶é•¿è®­ç»ƒï¼‰")
    
    # OneCycleLR å‚æ•°ï¼ˆå·²åœ¨ create_scheduler ä¸­è®¾ç½®ï¼‰
    
    # CosineAnnealingWarmRestarts å‚æ•°
    parser.add_argument("--cosine_t0", type=int, default=50,
                       help="CosineRestart: ç¬¬ä¸€ä¸ªå‘¨æœŸçš„ epoch æ•°")
    parser.add_argument("--cosine_tmult", type=int, default=1,
                       help="CosineRestart: å‘¨æœŸé•¿åº¦å€å¢å› å­")
    
    # ReduceLROnPlateau å‚æ•°
    parser.add_argument("--plateau_factor", type=float, default=0.5,
                       help="ReduceLROnPlateau: å­¦ä¹ ç‡è¡°å‡å› å­")
    parser.add_argument("--plateau_patience", type=int, default=10,
                       help="ReduceLROnPlateau: å®¹å¿å¤šå°‘ epoch ä¸æ”¹å–„")
    
    # ExponentialLR å‚æ•°
    parser.add_argument("--exp_gamma", type=float, default=0.95,
                       help="ExponentialLR: å­¦ä¹ ç‡è¡°å‡å› å­")
    
    # é€šç”¨è°ƒåº¦å™¨å‚æ•°
    parser.add_argument("--min_lr", type=float, default=1e-7,
                       help="æœ€å°å­¦ä¹ ç‡")
    
    # å¾®è°ƒæ¨¡å¼
    parser.add_argument("--finetune", action="store_true",
                       help="å¾®è°ƒæ¨¡å¼ï¼ˆä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼‰")
    parser.add_argument("--finetune_lr_ratio", type=float, default=0.1,
                       help="å¾®è°ƒå­¦ä¹ ç‡æ¯”ä¾‹ï¼ˆç›¸å¯¹äºåŸå­¦ä¹ ç‡ï¼‰")
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--save_dir", type=str, default="checkpoints")
    parser.add_argument("--log_dir", type=str, default="logs")
    
    # æ¢å¤è®­ç»ƒ
    parser.add_argument("--resume", type=str, default=None, 
                       help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--resume_epoch", type=int, default=None,
                       help="ä»æŒ‡å®š epoch æ¢å¤ï¼ˆè¦†ç›–æ£€æŸ¥ç‚¹ä¸­çš„ epochï¼‰")
    parser.add_argument("--ignore_config_mismatch", action="store_true",
                       help="å¿½ç•¥é…ç½®ä¸åŒ¹é…çš„è­¦å‘Šï¼ˆä¸æ¨èï¼‰")
    
    args = parser.parse_args()
    
    device_type = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device_type)
    
    # ========== å¤„ç†æ¢å¤è®­ç»ƒ ==========
    start_epoch = 1
    best_val_loss = float('inf')
    checkpoint = None
    
    if args.resume:
        checkpoint, model_config, training_config = load_checkpoint(args.resume, device)
        
        # éªŒè¯æ¨¡å‹é…ç½®
        model_match, model_mismatches = verify_config_match(args, model_config, 'model')
        if not model_match:
            print("âš ï¸  è­¦å‘Šï¼šæ¨¡å‹é…ç½®ä¸åŒ¹é…ï¼")
            print("ä¸åŒ¹é…çš„å‚æ•°:")
            for mismatch in model_mismatches:
                print(f"  {mismatch['param']}: å½“å‰={mismatch['current']}, ä¿å­˜={mismatch['saved']}")
            
            if not args.ignore_config_mismatch:
                print("\nâŒ æ¨¡å‹é…ç½®å¿…é¡»åŒ¹é…æ‰èƒ½åŠ è½½æƒé‡ï¼")
                print("è§£å†³æ–¹æ¡ˆ:")
                print("  1. ä½¿ç”¨ä¿å­˜çš„é…ç½®å‚æ•°é‡æ–°è¿è¡Œ")
                print("  2. ä½¿ç”¨ --ignore_config_mismatch å¼ºåˆ¶åŠ è½½ï¼ˆå¯èƒ½å¯¼è‡´é”™è¯¯ï¼‰")
                sys.exit(1)
            else:
                print("âš ï¸  ä½¿ç”¨ --ignore_config_mismatchï¼Œå¼ºåˆ¶ä½¿ç”¨å½“å‰é…ç½®")
                # ä½¿ç”¨ä¿å­˜çš„é…ç½®åˆ›å»ºæ¨¡å‹
                for key, value in model_config.items():
                    setattr(args, key, value)
        else:
            # ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹é…ç½®
            for key, value in model_config.items():
                setattr(args, key, value)
        
        # éªŒè¯è®­ç»ƒé…ç½®ï¼ˆä»…è­¦å‘Šï¼‰
        training_match, training_mismatches = verify_config_match(args, training_config, 'training')
        if not training_match:
            print("\nâ„¹ï¸  è®­ç»ƒé…ç½®æœ‰å˜åŒ–:")
            for mismatch in training_mismatches:
                print(f"  {mismatch['param']}: å½“å‰={mismatch['current']}, ä¿å­˜={mismatch['saved']}")
            print("å°†ä½¿ç”¨å½“å‰çš„è®­ç»ƒé…ç½®ç»§ç»­è®­ç»ƒ\n")
    
    # ========== æ‰“å°é…ç½® ==========
    print("="*70)
    print("Flow Matching ç®€ç¬”ç”»ä¸Šè‰²è®­ç»ƒ")
    print("="*70)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"\næ¨¡å‹é…ç½®:")
    print(f"  Base Channels: {args.base_channels}")
    print(f"  Time Embedding Dim: {args.time_emb_dim}")
    print(f"  Num Heads: {args.num_heads}")
    print(f"  Dropout: {args.dropout}")
    print(f"  Stochastic Depth: {args.stochastic_depth}")
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  å›¾åƒå¤§å°: {args.image_size}")
    print(f"  è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"  å­¦ä¹ ç‡: {args.lr}")
    print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {args.accumulation_steps}")
    print(f"  ä½¿ç”¨ EMA: {args.use_ema}")
    print(f"  è°ƒåº¦å™¨ç±»å‹: {args.scheduler_type}")
    if args.finetune:
        print(f"  å¾®è°ƒæ¨¡å¼: æ˜¯ (lr ratio: {args.finetune_lr_ratio})")
    print("="*70 + "\n")
    
    # æ£€æŸ¥æ•°æ®é›†
    if not check_and_download_dataset(args.data_dir):
        return
    
    # åŠ è½½æ•°æ®
    color_transform, sketch_transform = get_transforms(args.image_size)
    
    train_dataset = SketchColorPairDataset(
        root_dir=args.data_dir,
        split='train',
        sketch_method=args.sketch_method,
        use_cache=True,
        color_transform=color_transform,
        sketch_transform=sketch_transform
    )
    
    val_dataset = SketchColorPairDataset(
        root_dir=args.data_dir,
        split='val',
        sketch_method=args.sketch_method,
        use_cache=True,
        color_transform=color_transform,
        sketch_transform=sketch_transform
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}\n")
    
    # å¯è§†åŒ–æ ·æœ¬ï¼ˆä»…åœ¨æ–°è®­ç»ƒæ—¶ï¼‰
    if not args.resume:
        visualize_pairs(train_loader, num_samples=8, save_path='train_samples.png')
    
    # åˆ›å»ºæ¨¡å‹
    model = AdvancedFlowMatchingModel(
        base_channels=args.base_channels,
        time_emb_dim=args.time_emb_dim,
        num_heads=args.num_heads,
        dropout=args.dropout,
        stochastic_depth=args.stochastic_depth
    ).to(device)
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\n")
    
    # ä¼˜åŒ–å™¨ (ä½¿ç”¨ AdamW)
    optimizer = AdamW(
        model.parameters(), 
        lr=args.lr,
        weight_decay=0.01,
        betas=(0.9, 0.999)
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler()
    
    # æ„ŸçŸ¥æŸå¤±
    perceptual_loss = PerceptualLoss().to(device)
    
    # EMA
    ema = EMA(model, decay=0.9999) if args.use_ema else None
    
    # ========== åŠ è½½æ£€æŸ¥ç‚¹çŠ¶æ€ ==========
    if checkpoint is not None:
        # åŠ è½½æ¨¡å‹
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ“ æ¨¡å‹å‚æ•°å·²åŠ è½½")
        
        # åŠ è½½ä¼˜åŒ–å™¨
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("âœ“ ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
        
        # åŠ è½½ GradScaler
        if 'scaler_state_dict' in checkpoint:
            scaler.load_state_dict(checkpoint['scaler_state_dict'])
            print("âœ“ GradScaler çŠ¶æ€å·²åŠ è½½")
        
        # åŠ è½½ EMA
        if ema is not None and 'ema_shadow' in checkpoint:
            ema.shadow = checkpoint['ema_shadow']
            if 'ema_decay' in checkpoint:
                ema.decay = checkpoint['ema_decay']
            print("âœ“ EMA å‚æ•°å·²åŠ è½½")
        
        # æ¢å¤ epoch
        start_epoch = checkpoint['epoch'] + 1
        if args.resume_epoch is not None:
            start_epoch = args.resume_epoch
            print(f"âš ï¸  æ‰‹åŠ¨è®¾ç½®èµ·å§‹ epoch ä¸º {start_epoch}")
        
        # æ¢å¤æœ€ä½³éªŒè¯æŸå¤±
        if 'val_loss' in checkpoint:
            best_val_loss = checkpoint['val_loss']
            print(f"âœ“ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        
        # å¾®è°ƒæ¨¡å¼
        if args.finetune:
            print(f"\n{'='*70}")
            print("ğŸ¯ å¾®è°ƒæ¨¡å¼")
            print(f"{'='*70}")
            finetune_lr = args.lr * args.finetune_lr_ratio
            for param_group in optimizer.param_groups:
                param_group['lr'] = finetune_lr
            print(f"å¾®è°ƒå­¦ä¹ ç‡: {finetune_lr:.2e} (åŸå§‹ lr çš„ {args.finetune_lr_ratio:.1%})")
            print(f"{'='*70}\n")
    
    # ========== åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
    scheduler_needs_metric = False
    
    if args.resume and not args.reset_scheduler and not args.finetune:
        # å°è¯•åŠ è½½åŸè°ƒåº¦å™¨
        if 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:
            # å…ˆåˆ›å»ºåŒç±»å‹çš„è°ƒåº¦å™¨
            scheduler, scheduler_needs_metric = create_scheduler(
                optimizer, args, train_loader, start_epoch
            )
            try:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                print(f"âœ“ å­¦ä¹ ç‡è°ƒåº¦å™¨å·²åŠ è½½ï¼Œå½“å‰ lr: {optimizer.param_groups[0]['lr']:.2e}\n")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è°ƒåº¦å™¨å¤±è´¥: {e}")
                print("å°†åˆ›å»ºæ–°çš„è°ƒåº¦å™¨\n")
                scheduler, scheduler_needs_metric = create_scheduler(
                    optimizer, args, train_loader, start_epoch
                )
        else:
            scheduler, scheduler_needs_metric = create_scheduler(
                optimizer, args, train_loader, start_epoch
            )
    else:
        # åˆ›å»ºæ–°è°ƒåº¦å™¨
        if args.reset_scheduler:
            print("âš ï¸  é‡ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨\n")
        if args.finetune:
            # å¾®è°ƒæ¨¡å¼ä½¿ç”¨æŒ‡æ•°è¡°å‡
            print("ä½¿ç”¨å¾®è°ƒä¸“ç”¨çš„æŒ‡æ•°è¡°å‡è°ƒåº¦å™¨")
            scheduler = ExponentialLR(optimizer, gamma=0.95)
            scheduler_needs_metric = False
            print(f"âœ“ ExponentialLR: gamma=0.95\n")
        else:
            scheduler, scheduler_needs_metric = create_scheduler(
                optimizer, args, train_loader, start_epoch
            )
    
    if checkpoint is not None:
        print(f"âœ“ å°†ä» Epoch {start_epoch} ç»§ç»­è®­ç»ƒ")
        print(f"{'='*70}\n")
    
    # TensorBoard
    log_dir = Path(args.log_dir)
    log_dir.mkdir(exist_ok=True)
    writer = SummaryWriter(log_dir=log_dir)
    
    # ä¿å­˜ç›®å½•
    save_dir = Path(args.save_dir)
    save_dir.mkdir(exist_ok=True)
    
    # è®­ç»ƒ
    try:
        for epoch in range(start_epoch, args.num_epochs + 1):
            print(f"\n{'='*70}")
            print(f"Epoch {epoch}/{args.num_epochs}")
            print(f"{'='*70}")
            print(f"å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
            
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler, scaler, device, epoch,
                perceptual_loss, ema, args.accumulation_steps, 
                scheduler_needs_metric, writer
            )
            
            # ä½¿ç”¨ EMA æ¨¡å‹è¿›è¡ŒéªŒè¯
            if ema is not None:
                ema.apply_shadow()
            
            val_loss = validate(model, val_loader, device, perceptual_loss, epoch, writer)
            
            if ema is not None:
                ema.restore()
            
            # å¯¹äº ReduceLROnPlateauï¼Œéœ€è¦ä¼ å…¥ val_loss
            if scheduler is not None and scheduler_needs_metric:
                scheduler.step(val_loss)
            
            print(f"\nè®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
                save_checkpoint(
                    model, optimizer, scheduler, scaler, ema, epoch,
                    train_loss, val_loss, args,
                    save_dir / "best_model.pth",
                    is_best=True
                )
                print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f})")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 10 == 0:
                save_checkpoint(
                    model, optimizer, scheduler, scaler, ema, epoch,
                    train_loss, val_loss, args,
                    save_dir / f"checkpoint_epoch_{epoch}.pth",
                    is_best=False
                )
                
                # å¯è§†åŒ–ç»“æœ
                with torch.no_grad():
                    if len(val_loader) > 0:
                        sketch_samples, color_samples = next(iter(val_loader))
                        sketch_samples = sketch_samples[:4].to(device)
                        color_samples = color_samples[:4].to(device)
                        
                        writer.add_images('val/sketches', (sketch_samples + 1) / 2, epoch)
                        writer.add_images('val/ground_truth', (color_samples + 1) / 2, epoch)
        
        print("\n" + "="*70)
        print("âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"æ¨¡å‹ä¿å­˜åœ¨: {save_dir}")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n\nè®­ç»ƒè¢«ä¸­æ–­ï¼Œä¿å­˜å½“å‰æ¨¡å‹...")
        save_checkpoint(
            model, optimizer, scheduler, scaler, ema, epoch,
            train_loss, val_loss, args,
            save_dir / "interrupted_checkpoint.pth",
            is_best=False
        )
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir / 'interrupted_checkpoint.pth'}")
    
    finally:
        writer.close()


if __name__ == '__main__':
    main()
